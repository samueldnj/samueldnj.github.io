<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Samuel D N Johnson on Samuel D N Johnson</title>
    <link>/</link>
    <description>Recent content in Samuel D N Johnson on Samuel D N Johnson</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0700</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The effects of posterior sampling design on management procedure performance in MSE</title>
      <link>/talk/safs_qseminar_jan2019/safs_qseminar_jan2019/</link>
      <pubDate>Fri, 25 Jan 2019 12:30:00 -0800</pubDate>
      
      <guid>/talk/safs_qseminar_jan2019/safs_qseminar_jan2019/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.&lt;/p&gt;

&lt;/div&gt;


&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s &lt;em&gt;Slides&lt;/em&gt; feature and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Estimating a von Bertalanffy growth model from height measurements of my son during his first year of life</title>
      <link>/post/baby-growth/baby-growth/</link>
      <pubDate>Sat, 12 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/baby-growth/baby-growth/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;we-made-it-a-whole-year&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;We made it a whole year!&lt;/h1&gt;
&lt;p&gt;Our son just turned 1 year old!! We made it through the first year, with all its trials, twists and turns. We’re all happier, older, more full of love, and I hope a little bit wiser.&lt;/p&gt;
&lt;p&gt;To celebrate, I’m going to prove how much of a nerdy PhDad I am. I’m going to spend my saturday night fitting growth models to his height and weight observations we took at the various doctors’ appointments last year, instead of working on my thesis or a talk I’m giving next week.&lt;/p&gt;
&lt;p&gt;Really, I’m interested in 2 things: (1) trying to fit the data with a model, and (2) trying to predict my son’s adult height. &lt;em&gt;I’m envisioning this as a series of blog posts that I update with new data points&lt;/em&gt;. Ideally, I would be able to update my model fit as time goes on, and if this website persists long enough, I would be able to (in)validate my model with the true observations in about 18-22 years. Hah!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data&lt;/h1&gt;
&lt;p&gt;First, we need the data. I used a handy phone app to record heights and weights whenever they were observed at the various appointments. These data have been collected into the table at the bottom of this post. This shows the observations of height and weight (if the were taken) on julian day &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; since his birthday (&lt;span class=&#34;math inline&#34;&gt;\(j = 1\)&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;We can visualise the data on a multi-panel plot, and fit a smoother to each quantity to take a look at the average behaviour. I’ve done this below. Notice that we have a lot more weight observations than height observations in the first four months. All I can say about that is breastfeeding is &lt;strong&gt;HARD&lt;/strong&gt;, and my wife is a &lt;strong&gt;&lt;em&gt;rockstar&lt;/em&gt;&lt;/strong&gt;. The other thing you might notice is that this data follows a fairly steady trend. The deviations in the height plots are likely observation errors (height data for infants is noisy). On the other hand, the deviations for weight observations are likely process variation, as doctors’ offices have fairly accurate scales that take an average weight over a time period; this variation can be explained by changes in my son’s diet and the occasional growth spurt.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/baby-growth/baby-growth_files/figure-html/unnamed-chunk-2-1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling-human-growth&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Modeling human growth&lt;/h1&gt;
&lt;p&gt;There are several papers on modeling human growth in the medical literature that I just turned up in a quick google scholar search. Most are based on the Infancy-Childhood-Puberty (ICP) model &lt;span class=&#34;citation&#34;&gt;(Karlberg 1987)&lt;/span&gt;, which seems to have been the standard for most of the last 30 years. Recently, an updated approach was derived based on phenomenological universalities &lt;span class=&#34;citation&#34;&gt;(Gliozzi et al. 2012)&lt;/span&gt;, but to my untrained eye this model overlaps a lot with the ICP model.&lt;/p&gt;
&lt;p&gt;I’m going to use a different model, though. You might have guessed it, given that I’m a fisheries student, but I’m going to use a von Bertalanffy model, which I will refer to as the vonB model for short. This is because neither of the above models from the medical literature satisfy both of my requirements for this post. They’ll certainly fit to my son’s data (who knows how well - maybe another post) but they won’t predict adult height from infant observations given that they switch model structures for different life stages, so I’m going to pass on them.&lt;/p&gt;
&lt;div id=&#34;defining-the-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Defining the model&lt;/h2&gt;
&lt;p&gt;I’m going to use the form of the vonB model that de-correlates the asymptotic (adult) length &lt;span class=&#34;math inline&#34;&gt;\(L_{\infty}\)&lt;/span&gt; from the growth ‘rate’&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; &lt;span class=&#34;citation&#34;&gt;(Schnute 1981; Francis 2016)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
L_a = L_1 + (L_2 - L_1) \cdot \frac{e^{-K A_1} - e^{-K a}}{e^{-K A_1} - e^{-K A_2}}.
\]&lt;/span&gt;
Here, &lt;span class=&#34;math inline&#34;&gt;\(L_a\)&lt;/span&gt; is the length at age &lt;span class=&#34;math inline&#34;&gt;\(a\)&lt;/span&gt;, in years, &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; is the rate of change in length per unit time, and &lt;span class=&#34;math inline&#34;&gt;\(L_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(L_2\)&lt;/span&gt; are the lengths at ages &lt;span class=&#34;math inline&#34;&gt;\(A_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(A_2\)&lt;/span&gt;, ages that are chosen to be representative of early and late stage lengths, respectively.&lt;/p&gt;
&lt;p&gt;I’m going to have to modify this model a little bit to use the data I have. First, humans stand up, so I’m modeling height &lt;span class=&#34;math inline&#34;&gt;\(H\)&lt;/span&gt;, not growth &lt;span class=&#34;math inline&#34;&gt;\(L\)&lt;/span&gt; - a completely aesthetic changes of variables. Second, I have observations with time-steps in days, not years, so I’m going to have to use a different rate &lt;span class=&#34;math inline&#34;&gt;\(K&amp;#39; = K/365\)&lt;/span&gt;, making the formula
&lt;span class=&#34;math display&#34;&gt;\[
H_d = H_1 + (H_2 - H_1) \cdot \frac{e^{-K&amp;#39; D_1} - e^{-K&amp;#39; d}}{e^{-K&amp;#39; D_1} - e^{-K&amp;#39; D_2}},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(D_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D_2\)&lt;/span&gt; are ages in days for &lt;span class=&#34;math inline&#34;&gt;\(H_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(H_2\)&lt;/span&gt;, respectively.&lt;/p&gt;
&lt;p&gt;I should be able to estimate this model from the data, but I foresee at least one challenge. This is a three parameter model (&lt;span class=&#34;math inline&#34;&gt;\(L_1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(L_2\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;). I &lt;em&gt;technically&lt;/em&gt; have enough data to estimate three parameters, but I don’t have any data that represents the late stage height &lt;span class=&#34;math inline&#34;&gt;\(H_2\)&lt;/span&gt; (we’ve got some time to wait for that). I’m going to attempt extrapolating out to 18 years, by setting &lt;span class=&#34;math inline&#34;&gt;\(D_2 = 365\)&lt;/span&gt; to see what it does, and encouraging the model to 20 years by setting a prior distribution on the &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; parameter, with a mean value intended to produce something close to a heuristic prediction of adult height. My wife is 158 cm, and I am 178cm, so our son’s predicted maximum height according to &lt;a href=&#34;https://www.mayoclinic.org/healthy-lifestyle/childrens-health/expert-answers/child-growth/faq-20057990&#34;&gt;this Mayo Clinic method&lt;/a&gt; is
&lt;span class=&#34;math display&#34;&gt;\[
\mu_{H_\infty} = \frac{158 + 178 + 13}{2} = 174.5 \mbox{cm}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;implementing-the-vonb-model-r&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Implementing the vonB model R&lt;/h1&gt;
&lt;p&gt;So, let’s get into the knitty-gritty. We’re going to optimise later, so I’m going to define a &lt;code&gt;vonB()&lt;/code&gt; function, which takes the number of days &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and the model parameters defined above as arguemnts. I’m going to set function default argument values based on the observations, with &lt;span class=&#34;math inline&#34;&gt;\(D_1 = 100\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(D_2 = 365\)&lt;/span&gt;, and assume a yearly &lt;span class=&#34;math inline&#34;&gt;\(K = 0.25\)&lt;/span&gt; for now.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First, create a function to calculate the 
# height at a given day d. Leading parameters 
# H_1, H_2 and K are separated from D values 
# as pars are estimated model parameters passed
# in from the log-likelihood function
vonB &amp;lt;- function( d = 1, 
                  pars = c( H_1 = 62.5, 
                            H_2 = 74.5, 
                            K = 0.25 ), 
                  D = c(100,365) )
{
  # Recover pars
  H_1 &amp;lt;- pars[1]
  H_2 &amp;lt;- pars[2]
  K   &amp;lt;- pars[3]/365

  # Recover D values
  D_1 &amp;lt;- D[1]
  D_2 &amp;lt;- D[2]
  
  # Run calculation
  H_d &amp;lt;- H_1 + (H_2 - H_1) * (exp(-K*D_1) - exp(-K*d)) /( exp(-K*D_1) - exp(-K*D_2) )

  # Return H_d
  return(H_d)
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;fitting-by-eye&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fitting by eye&lt;/h2&gt;
&lt;p&gt;If I run the model with the default values, we can see that it fits the data we have pretty well by eye. The only caveat is that the asymptotic height is 134.79cm, which is low based on my heuristic estimate of 174.5cm above. This is likely because my randomly chosen initial &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; value is too high, leading the growth to saturate at a lower asymptotic height.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/baby-growth/baby-growth_files/figure-html/defaultFit-1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Thanks to my functional workflow, it’s easy to change values and produce more growth curves.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;H2 &amp;lt;- vonB( d = 1:14600, pars = c(  H_1 = 62.5, 
                                    H_2 = 74.5, 
                                    K = 0.16 )  )
Hinf2 &amp;lt;- max(H2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After some fiddling around, I can get a better asymptotic height from &lt;span class=&#34;math inline&#34;&gt;\(K = 0.16\)&lt;/span&gt;, which produces an asymptotic height of 171.73cm. I’ve plotted this new curve and the previous model on the axes below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/baby-growth/baby-growth_files/figure-html/handFitPlot-1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;optimisation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Optimisation&lt;/h2&gt;
&lt;p&gt;Let’s optimise! First, we need a negative log-posterior density function, which we’re going to use in an &lt;code&gt;optim()&lt;/code&gt; call. We’ll run the optimisation with the default values, which is a short interval between &lt;span class=&#34;math inline&#34;&gt;\(D_1 = 100\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(D_2 = 365\)&lt;/span&gt;. I’m going to add three prior distributions to penalise deviations from (1) the last observed height of 74.5cm at 1 year old, (2) the &lt;span class=&#34;math inline&#34;&gt;\(K = 0.16\)&lt;/span&gt; growth rate that I eye-balled to give an adult height similar to the heuristic from the Mayo Clinic, and (3) an improper Jeffreys prior on the observation error &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Assuming the residuals between the model and the data are normally distributed, we have the likelihood function
&lt;span class=&#34;math display&#34;&gt;\[
\mathcal{L} = \prod_{i} \frac{1}{2\sqrt{\pi\sigma^2}} e^{-\frac{(H_d - \hat{H}_d)^2}{2\sigma^2}}.
\]&lt;/span&gt;
This is implemented in the following code chunk, along with the priors on &lt;span class=&#34;math inline&#34;&gt;\(H_2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Define negative log posterior function,
# we&amp;#39;re going to model the H1, H2 and K
# values on the log scale, and we need to
# model
vonB_negLogPost &amp;lt;- function(  theta = c( logH_1 = log(62.5), 
                                         logH_2 = log(74.5), 
                                         logK = log(0.16),
                                         logsigma = 0 ),
                              data = growthData,
                              D = c(100,365),
                              H2Prior = c(74.5, 74.5),
                              KPrior = c(.16, .16/2) )
{
  # Exponentiate leading pars of vonB
  pars &amp;lt;- c(  H_1 = exp(theta[1]),
              H_2 = exp(theta[2]),
              K   = exp(theta[3]) )

  # uncertainty in the observations
  sigma &amp;lt;- exp(theta[&amp;quot;logsigma&amp;quot;])

  # Calculate expected observations in a new column
  # of the data frame, then residuals,
  # and then the negative-log-likelihood (with
  # normalising scalar omitted)
  data &amp;lt;- data %&amp;gt;%
          mutate( expHeight = vonB(j, pars = pars, D = D),
                  resid = expHeight - height,
                  negloglik = 0.5*log(sigma^2) + 0.5 * resid^2/sigma^2 )


  # Calculate observation NLL and H2/K neg log priors
  nll     &amp;lt;- sum(data$negloglik, na.rm = T)
  nlp_H2  &amp;lt;- 0
  nlp_K   &amp;lt;- 0
  # Only calculate priors if parameters are given
  if(!is.null(H2Prior))
    nlp_H2  &amp;lt;- 0.5 * (pars[2] - H2Prior[1])^2/H2Prior[2]^2
  if(!is.null(KPrior))
    nlp_K   &amp;lt;- 0.5 * (pars[3] - KPrior[1])^2/KPrior[2]^2


  objFun  &amp;lt;- nll + nlp_H2 + nlp_K + 10 * log(sigma) 

  return(objFun)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’m going to fit using &lt;code&gt;optim()&lt;/code&gt; as my optimiser, with the Nelder-Mead optimisation method. The Nelder-Mead method is a &lt;em&gt;direct-search&lt;/em&gt; method, as opposed to more common quasi-Newton methods of optimisation (see &lt;a href=&#34;https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method&#34;&gt;Wikipedia&lt;/a&gt;), which basically means that it uses the value of the objective function only, and no derivatives of the objective function. This makes it a little more robust to certain objective functions, albeit somewhat slower. I chose this because I have no need of the covariance matrix as an output (for now), which is one of the drawbacks of Nelder-Mead.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fit with no priors
fit1 &amp;lt;- optim(  par = c(  logH_1 = log(62.5), 
                           logH_2 = log(74.5), 
                           logK = log(0.16/365),
                           logsigma = 0),
                fn = vonB_negLogPost,
                D = c(100,365), data = growthData,
                H2Prior = NULL,
                KPrior = NULL,
                method = &amp;quot;Nelder-Mead&amp;quot; )



# Fit with priors on K
fit2 &amp;lt;- optim(  par = c(  logH_1 = log(62.5), 
                           logH_2 = log(74.5), 
                           logK = log(0.16/365),
                           logsigma = 0),
                fn = vonB_negLogPost,
                D = c(100,365), data = growthData,
                H2Prior = NULL,
                KPrior = c(0.16,0.08),
                method = &amp;quot;Nelder-Mead&amp;quot; )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The parameters of the two models are given in the following table, and the plots are shown below that. We can see that the largest&lt;/p&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;width: auto !important; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:optFitPlot&#34;&gt;Table 1: &lt;/span&gt;Optimised parameters, adult height and negativ log-posterior values under both models.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;span class=&#34;math display&#34;&gt;\[H_1\]&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;span class=&#34;math display&#34;&gt;\[H_2\]&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;span class=&#34;math display&#34;&gt;\[K\]&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;span class=&#34;math display&#34;&gt;\[\sigma\]&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
&lt;span class=&#34;math display&#34;&gt;\[H_\infty\]&lt;/span&gt;
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
-log(Post)
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
No Priors
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
61.96
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
75.81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.57
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.85
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
102.83
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.363659
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Priors
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
62.26
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
76.71
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.18
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.95
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
180.12
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.692054
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;/post/baby-growth/baby-growth_files/figure-html/optFitPlot-1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;The model with no priors shows an adult (asymptotic) height of &lt;span class=&#34;math inline&#34;&gt;\(H_\infty = 102.83\)&lt;/span&gt;, meaning that my son is forecast by this model to be a little bit over 1 metre tall. Looking at the graph, we can see that he’s going to reach that height at about 2000 days, or 5.5 years old, influenced by a growth rate parameter of &lt;span class=&#34;math inline&#34;&gt;\(K = 0.57\)&lt;/span&gt;. Given that both my wife and I are taller than this, and we know of no reasons why we would expect him to be under the first percentile for height, I don’t fully believe this model. On the other hand, we have the model with priors predicting my son will reach his adult height of &lt;span class=&#34;math inline&#34;&gt;\(H_\infty = 180.12\)&lt;/span&gt; at about 20 years of age, with a much lower growth rate &lt;span class=&#34;math inline&#34;&gt;\(K = 0.18\)&lt;/span&gt;. Although the second model is guided by subjective prior distributions, I believe it more - not only because it’s almost exactly my height, but also because aside from the value of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; the model parameters are very close, and the negative-log-posterior values are also very close.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;table class=&#34;table table-striped table-hover table-condensed table-responsive&#34; style=&#34;width: auto !important; float: right; margin-left: 10px;&#34;&gt;
&lt;caption&gt;
&lt;span id=&#34;tab:unnamed-chunk-3&#34;&gt;Table 2: &lt;/span&gt;Data for my son’s health visits.
&lt;/caption&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
j
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
height
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
weight
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
56.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.8
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
15
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
18
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
25
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
29
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
39
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4.9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
46
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
50
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
59.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
59
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.1
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
74
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
81
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.4
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
86
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
60.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
101
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
62.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5.9
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
122
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
64.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
192
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
68.0
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7.0
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
276
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
74.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.3
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
296
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
71.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8.5
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
373
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
74.5
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9.6
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, there we go. I fit a von Bertalanffy growth model to height observations from my son’s health appointments over the first year of his life (data to the right). My aim was to (1) see how well human growth data fit the vonB mode, and (2) predict his adult height from observations now. I found that the vonB model fits human growth data very well, at least during the first year of life where a single growth regime is present &lt;span class=&#34;citation&#34;&gt;(Karlberg 1987)&lt;/span&gt;. On the other hand, I found extrapolating beyond the first year problematic, and required subjective prior distributions on leading parameters to achieve a sensible height prediction.&lt;/p&gt;
&lt;p&gt;The high growth rate of infants is likely the reason that the model without priors estimates a high &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; value, reaching &lt;span class=&#34;math inline&#34;&gt;\(H_\infty\)&lt;/span&gt; sometime during childhood. According to the ICP model, infants have a high but rapidly decreasing growth rate, which is replaced by a lower, more stable, rate in childhood, and another different rate during puberty, before adult height is reached &lt;span class=&#34;citation&#34;&gt;(Karlberg 1987)&lt;/span&gt;. Later stages have a “humpy” growth pattern, where humps correspond to growth spurts during childhood and puberty.&lt;/p&gt;
&lt;p&gt;Further work could be conducted to generate uncertainty in my prediction. I used a penalised likelihood method to estimate the parameters of the vonB model, with prior distributions penalising deviations from the observed height at 1 year old, and deviations from a growth rate that would produce an adult height that was similar to a weighted average of my wife’s and my own heights. Parameter estimates and the forecast are all maximum likelihood estimates (posterior modes), with no uncertainty reported. Confidence intervals could be estimated by using the delta-method, bootstrapping the residuals on each observation, or by running a Markov-Chain Monte-Carlo numerical integration of the posterior to generate full posterior distributions of all parameters and the model.&lt;/p&gt;
&lt;p&gt;That’s all from me. Happy birthday to my son, thanks to my wife for growing him and being an awesome partner, and thanks to you for reading this nerdy Dad rant!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-francis2016growth&#34;&gt;
&lt;p&gt;Francis, RIC Chris. 2016. “Growth in Age-Structured Stock Assessment Models.” &lt;em&gt;Fisheries Research&lt;/em&gt; 180. Elsevier: 77–86.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gliozzi2012novel&#34;&gt;
&lt;p&gt;Gliozzi, Antonio S, Caterina Guiot, Pier Paolo Delsanto, and Dan A Iordache. 2012. “A Novel Approach to the Analysis of Human Growth.” &lt;em&gt;Theoretical Biology and Medical Modelling&lt;/em&gt; 9 (1). BioMed Central: 17.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-karlberg1987modelling&#34;&gt;
&lt;p&gt;Karlberg, Johan. 1987. “On the Modelling of Human Growth.” &lt;em&gt;Statistics in Medicine&lt;/em&gt; 6 (2). Wiley Online Library: 185–92.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-schnute1981versatile&#34;&gt;
&lt;p&gt;Schnute, Jon. 1981. “A Versatile Growth Model with Statistically Stable Parameters.” &lt;em&gt;Canadian Journal of Fisheries and Aquatic Sciences&lt;/em&gt; 38 (9). NRC Research Press: 1128–40.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;yes, I know it’s not technically a rate&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Curriculum Vitae</title>
      <link>/cv/cv/</link>
      <pubDate>Thu, 10 Jan 2019 00:00:00 -0800</pubDate>
      
      <guid>/cv/cv/</guid>
      <description>

&lt;p&gt;Download a &lt;a href=&#34;http://sdnjohnson.com/files/JohnsonCV.pdf&#34; target=&#34;_blank&#34;&gt;PDF&lt;/a&gt; copy.&lt;/p&gt;

&lt;h2 id=&#34;employment&#34;&gt;Employment&lt;/h2&gt;

&lt;dl&gt;
&lt;dt&gt;2016 - Present: &lt;strong&gt;Sub-contracting Consultant&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Landmark Fisheries Research (Port Moody, BC)&lt;/dd&gt;
&lt;dd&gt;&lt;em&gt;Supervisors:&lt;/em&gt; Ashleen Benson, Sean Cox&lt;/dd&gt;
&lt;dt&gt;2016 - Present: &lt;strong&gt;MITACS Accelerate Intern&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Wild Canadian Sablefish (Steveston, BC)&lt;/dd&gt;
&lt;dd&gt;Pacific Halibut Management Association (Vancouver, BC)&lt;/dd&gt;
&lt;dd&gt;Canadian Groundfish Research and Conservation Society (New Westminster, BC)&lt;/dd&gt;
&lt;dd&gt;&lt;em&gt;Supervisors:&lt;/em&gt; Sean Cox, Chris Acheson, Chris Sporer, Bruce Turris&lt;/dd&gt;
&lt;dt&gt;2015 - 2016: &lt;strong&gt;MITACS Accelerate Intern&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Wild Canadian Sablefish (Steveston, BC)&lt;/dd&gt;
&lt;dd&gt;&lt;em&gt;Supervisors:&lt;/em&gt; Sean Cox, Chris Acheson&lt;/dd&gt;
&lt;dt&gt;2012 - 2014: &lt;strong&gt;Research Assistant&lt;/strong&gt;&lt;/dt&gt;
&lt;dd&gt;Department of Mathematics, Simon Fraser University (Burnaby, BC)&lt;/dd&gt;
&lt;dd&gt;&lt;em&gt;Supervisor:&lt;/em&gt; Marni Mishna&lt;/dd&gt;
&lt;/dl&gt;

&lt;h2 id=&#34;education&#34;&gt;Education&lt;/h2&gt;

&lt;dl&gt;
&lt;dt&gt;2014 - 2019 (Expected) &lt;em&gt;PhD, Resource and Environmental Management&lt;/em&gt;&lt;/dt&gt;
&lt;dd&gt;Simon Fraser University (Burnaby, BC)&lt;/dd&gt;
&lt;dd&gt;&lt;em&gt;Advisor:&lt;/em&gt; Associate Professor Sean Cox&lt;/dd&gt;
&lt;dd&gt;&lt;em&gt;Thesis title:&lt;/em&gt; Multispecies Management Tools for Multi Sector Fisheries&lt;/dd&gt;
&lt;dt&gt;2010 - 2012: &lt;em&gt;MSc, Mathematics&lt;/em&gt;&lt;/dt&gt;
&lt;dd&gt;Simon Fraser University (Burnaby, BC)&lt;/dd&gt;
&lt;dd&gt;&lt;em&gt;Advisor:&lt;/em&gt; Associate Professor Marni Mishna&lt;/dd&gt;
&lt;dd&gt;&lt;em&gt;Thesis title:&lt;/em&gt; &lt;a href=&#34;http://arxiv.org/pdf/1304.6432.pdf&#34; target=&#34;_blank&#34;&gt;Analytic Combinatorics of Planar Lattice Paths&lt;/a&gt;&lt;/dd&gt;
&lt;dt&gt;2005-2009: &lt;em&gt;B. Mathematics (Hons)&lt;/em&gt;&lt;/dt&gt;
&lt;dd&gt;University of Newcastle (Newcastle, NSW)&lt;/dd&gt;
&lt;dd&gt;&lt;em&gt;Honours Advisor:&lt;/em&gt; Associate Professor George Willis&lt;/dd&gt;
&lt;dd&gt;&lt;em&gt;Honours Thesis Title:&lt;/em&gt; Simple Groups of Automorphisms of Locally Finite Trees&lt;/dd&gt;
&lt;/dl&gt;

&lt;h2 id=&#34;technical-skills&#34;&gt;Technical Skills&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;I am proficient in the R statistical language, Rmarkdown, latex, Auto-Differentiation Model Builder (ADMB), and Template Model Builder (TMB)&lt;/li&gt;
&lt;li&gt;I have used SQL, Python, and C++&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;ve conducted closed loop simulations in a management strategy evaluation (MSE) framework for simulated and real fishery management systems, including Northern Cod (&lt;em&gt;Gadus morhua&lt;/em&gt;) in Atlantic Canada, and Sablefish (&lt;em&gt;Anoplopoma fimbria&lt;/em&gt;) and Herring (&lt;em&gt;Clupea pallasii&lt;/em&gt;) in Pacific Canada&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;m experienced in teaching technical material in both small and large classroom settings&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;publications&#34;&gt;Publications&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>A combinatorial understanding of lattice path asymptotics.</title>
      <link>/publication/latticepathasymptotics/</link>
      <pubDate>Sun, 16 Dec 2018 21:50:34 -0800</pubDate>
      
      <guid>/publication/latticepathasymptotics/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Using autonomous video to estimate the bottom-contact area of longline trap gear and presence–absence of sensitive benthic habitat</title>
      <link>/publication/bottomcontact/</link>
      <pubDate>Sun, 16 Dec 2018 21:50:24 -0800</pubDate>
      
      <guid>/publication/bottomcontact/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Testing Rmd Support using a Schafer model</title>
      <link>/post/test-rmd/test-rmd/</link>
      <pubDate>Mon, 17 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/test-rmd/test-rmd/</guid>
      <description>


&lt;p&gt;This is version 3 of my personal website. I’ve revised to a &lt;a href=&#34;&#34;&gt;Hugo&lt;/a&gt; powered &lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34;&gt;blogdown&lt;/a&gt; generated website for two reasons. First, it’s a static site generator, similar to Jekyll, serving to keep things simple and fast on the server side. Second, I’ve wanted to start blogging more, and I think the Rmarkdown integration with blogdown and Hugo will help smooth this out.&lt;/p&gt;
&lt;p&gt;This post is a test post for using Rmarkdown to build a simple population dynamics model with process error, and generate some data with observation error. Pushing this post to github pages will require working out how &lt;a href = http://www.travis-ci.com&gt;TravisCI&lt;/a&gt; works, which I learned from David Selby’s &lt;a href=&#34;https://selbydavid.com/2017/06/22/blogdown-travis/&#34;&gt;Tea and Stats&lt;/a&gt; blog - a handy resource.&lt;/p&gt;
&lt;p&gt;So, let’s get started by defining a function for the population dynamics model. I’m going to use a Schaefer surplus production model &lt;span class=&#34;citation&#34;&gt;(Schaefer 1957)&lt;/span&gt;. I like to define my Schafer models using optimal equilibrium values, known as &lt;span class=&#34;math inline&#34;&gt;\(B_{MSY}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(U_{MSY}\)&lt;/span&gt; in the fisheries literature, as the leading parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)

# Create a function to quickly call for generating biomass
prodMod &amp;lt;- function(  Umsy = 0.06, 
                      Bmsy = 42, 
                      procSigma = 0.1, 
                      obsSigma = 0.2,
                      nT = 100 )
{
  # Vectors to hold biomass and catch
  Bt &amp;lt;- numeric( length = nT )
  Ct &amp;lt;- numeric( length = nT )

  # Create a sequence of harvest rates
  Utmult &amp;lt;- c(seq(0.1,2,length = 20),seq(2,1, length = 20), rep(1,nT - 40) )
  Ut &amp;lt;- Utmult * Umsy

  # And draw process errors
  epst &amp;lt;- rnorm(nT-1, sd = procSigma)

  # Initialise at unfished
  Bt[1] &amp;lt;- 2 * Bmsy
  # Take first year&amp;#39;s catch
  Ct[1] &amp;lt;- Ut[1] * Bt[1]

  # Now loop and fill remaining years
  for( t in 2:nT )
  {
    # Generate non-stochastic biomass
    Bt[t] &amp;lt;- Bt[t-1] + 2 * Umsy * Bt[t-1] * (1 - Bt[t-1] / Bmsy / 2) - Ct[t-1]
    # Add process errors
    Bt[t] &amp;lt;- Bt[t] * exp( epst[t-1])
    # Take catch
    Ct[t] &amp;lt;- Bt[t] * Ut[t]
  }

  # Draw observation errors
  deltat &amp;lt;- rnorm(nT, sd = obsSigma)

  # Generate absolute index of biomass
  It &amp;lt;- Bt * exp(deltat)

  # Return biomass, catch and model time dimension
  outList &amp;lt;- list(  Bt = Bt,
                    It = It,
                    Ct = Ct,
                    nT = nT )
}

# Generate the biomass
popSP &amp;lt;- prodMod()

# Plot the biomass and catch
plot( x = 1:popSP$nT, y = popSP$Bt, xlab = &amp;quot;Year&amp;quot;, ylab = &amp;quot;Biomass and Catch&amp;quot;,
      ylim = c(0,max(popSP$Bt)), type = &amp;quot;l&amp;quot;, col = &amp;quot;red&amp;quot;, lwd = 2, las = 1  )
  rect( xleft = 1:popSP$nT-.3, xright = 1:popSP$nT + .3,
        ybottom = 0, ytop = popSP$Ct, col = &amp;quot;grey40&amp;quot;, border = NA )
  points( x = 1:popSP$nT, y = popSP$It, pch = 21, col = &amp;quot;grey60&amp;quot;,
          cex = .8 )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/test-Rmd/test-Rmd_files/figure-html/unnamed-chunk-1-1.png&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;Our figure shows a 2-way trip, which describes how the biomass declines at first (the first way), then comes back up as catch is decreased (the second way). This is an ideal data set for fisheries stock assessment, as there is contrast in the catch and biomass, making it informative about both the scale of the biomass, helping to estimate &lt;span class=&#34;math inline&#34;&gt;\(B_{MSY}\)&lt;/span&gt;, and the productivity of the stock, helping to estimate &lt;span class=&#34;math inline&#34;&gt;\(U_{MSY}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The next part of this post will fit a TMB model to the data generated from the above biomass. I ran this same tutorial in a seminar for our lab group at SFU this past fall, so I’ll be copying and pasting the code in from there.&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-schaefer1957some&#34;&gt;
&lt;p&gt;Schaefer, Milner B. 1957. “Some Considerations of Population Dynamics and Economics in Relation to the Management of the Commercial Marine Fisheries.” &lt;em&gt;Journal of the Fisheries Board of Canada&lt;/em&gt; 14 (5). NRC Research Press: 669–81.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Welcome!</title>
      <link>/post/first-post/first-post/</link>
      <pubDate>Sun, 16 Dec 2018 00:00:00 -0800</pubDate>
      
      <guid>/post/first-post/first-post/</guid>
      <description>

&lt;h1 id=&#34;first-post&#34;&gt;First Post!&lt;/h1&gt;

&lt;p&gt;Welcome to my blog! I plan to use this blog for two main purposes. The first is as an academic learning resource, where I will write posts that go through new concepts that I&amp;rsquo;m interested in learning about. The second is to develop new research ideas, with a view to refining them into primary publications.&lt;/p&gt;

&lt;p&gt;Most posts will have something to do with Fisheries Science, the field I&amp;rsquo;m actively involved in. Fisheries science is an applied discipline drawing on the fields of biology, population ecology, statistics, data science, decision analysis, and risk assessment just to name a few. I specialise in fisheries stock assessment modeling, which is the application of population dynamics for the determination of fish stock status. I am also interested in broadening my understanding of ecological theory and new applied tools for testing hypotheses, and improving my science communication skills through better writing, data visualisation.&lt;/p&gt;

&lt;p&gt;Thanks for visiting, and be sure to return for future posts with some more theoretical exposition.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evaluating the role of data quality when sharing information in hierarchical multi-stock assessment models, with an application to Dover Sole.</title>
      <link>/publication/hierprod/</link>
      <pubDate>Thu, 18 Oct 2018 21:46:24 -0800</pubDate>
      
      <guid>/publication/hierprod/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
